{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04f4690",
   "metadata": {},
   "source": [
    "## Binning Data: Mean and Error Bars\n",
    "\n",
    "We will now work a toy problem isnpired by the detection of Baryon Accoustic Oscilaltion's in the galacy-galaxy correlation function.  We will simulate a toy model for data with and without correlations, summarize these data with means and error bars, and then introduce and use $\\chi^2$ to fit these data and estimate uncertainties on the recovered paramters.\n",
    "\n",
    "\n",
    "The toy model for the correlation function will be the sum of a power law and a gaussian chosen to approximate the cosmological signal.   $$C(r) = A \\left( \\frac 1 {10} \\right)^{r/50} + B e^{- \\frac 1 2 \\left( \\frac {r-120} {10} \\right)^2 }.$$  In generating the model the amplitudes $A$ and $B$ will be fixed to 1 and 0.02 respectively.  When we fit the data we will allow $A$ and $B$ to vary while holding the remaining paratmers fixed.  This toy model will be used to demonstrate statistical technqiues.  This is a carton version of the cosmological measuremnt intened only to illustrate statistical methods.\n",
    "\n",
    "Our first step is to simulate mesauments of $C(r)$ by treating each bin in $r$ as containing some number of data points that sampel a gaussian distribtion centerd at $C(r)$.  For each bin we compute the mean and variance on the mean which are plotted as a simulated mesurmeent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51001db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a toy modle for the galaxy-galaxy corerlation funciton\n",
    "\n",
    "r = np.arange(190) + 10\n",
    "gal_gal_cor_empricial_theory = 1*(1/10.)**(r/50.)+ 0.02*np.exp(-0.5*((r-120)/10.)**2)\n",
    "plt.plot(r,gal_gal_cor_empricial_theory)\n",
    "plt.xlabel(\"r (MPc / h)\")\n",
    "plt.ylabel(\"correlation fucntion (arb)\")\n",
    "\n",
    "\n",
    "## make simulated data with error bars, no correlation\n",
    "bin_width = 10. # the sampling of the measurement\n",
    "N_pts_in_bin  = 1000 # numper of samples used to compute the mean and rms in each bin\n",
    "N_data_pts = 190 / bin_width  ## number of data points for the plot\n",
    "sigma_corr = 0.005 * np.sqrt(N_pts_in_bin)  ## uncertainty in the correlatoin fucntion\n",
    "\n",
    "i = 0\n",
    "while (i < N_data_pts):\n",
    "    r_bin  = bin_width*(i+1)\n",
    "    theory_mean = gal_gal_cor_empricial_theory[(np.where(r == r_bin))[0] ]\n",
    "    data_in_bin = np.random.randn(N_pts_in_bin)*sigma_corr + theory_mean\n",
    "    mean_pt = np.mean(data_in_bin)\n",
    "    error_pt = np.std(data_in_bin) / np.sqrt(N_pts_in_bin - 1)  ## this is the error on the mean\n",
    "    plt.errorbar(r_bin,mean_pt,error_pt,marker=',',mec='red',mfc='red',c=\"red\")\n",
    "    i+=1\n",
    "\n",
    "plt.title(\"uncorrelated\")\n",
    "plt.ylim(-.1,.2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629b739",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "why don't the central values (eg the squares) lie on the line? Re-run this code and see how varius relaizations look. increase N_pts_in_bin and see how the mean converges and the variance on the variance decreases.  Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802a87e",
   "metadata": {},
   "source": [
    "### Case with Correlated Noise\n",
    "\n",
    "We now generate simulated data with a correlation in the noise between neighboroing bins.  the default correlation is 0.9 betwene neighboring bins.  We will compare the plots of the data with correlations to those without to learn what correlated erorrs look like visually.  In subsequent stesps we will fit these data to recover estimtes for $A$ and $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a toy modle for the galaxy-galaxy corerlation funciton\n",
    "\n",
    "r = np.arange(190) + 10\n",
    "gal_gal_cor_empricial_theory = 1*(1/10.)**(r/50.)+ 0.02*np.exp(-0.5*((r-120)/10.)**2)\n",
    "plt.plot(r,gal_gal_cor_empricial_theory)\n",
    "plt.xlabel(\"r (MPc / h)\")\n",
    "plt.ylabel(\"correlation fucntion (arb)\")\n",
    "\n",
    "\n",
    "## make simulated data with error bars, no correlation\n",
    "bin_width = 10. # the sampling of the measurement\n",
    "N_pts_in_bin  = 1000 # numper of samples used to compute the mean and rms in each bin\n",
    "N_data_pts = int(190 / bin_width)  ## number of data points for the plot\n",
    "sigma_corr = 0.005*np.sqrt(N_pts_in_bin)  ## uncertainty in the correlatoin fucntion\n",
    "## new line\n",
    "correlatipon_fator = 0.9  ###<<<<<<  nearest neighbor correlation factor <<<<<<<<<<<<<<<<<\n",
    "\n",
    "## store all the data for future use\n",
    "Raw_data = np.zeros([N_data_pts,N_pts_in_bin])\n",
    "Binned_r = np.zeros(N_data_pts)\n",
    "Binned_mean = np.zeros(N_data_pts)\n",
    "Binned_error = np.zeros(N_data_pts)\n",
    "\n",
    "\n",
    "## set up arrays so we can create nearesst neighbor correlation in the loop below\n",
    "random_in_bin_last = np.random.randn(N_pts_in_bin)*sigma_corr + theory_mean\n",
    "random_in_bin = np.random.randn(N_pts_in_bin)*sigma_corr + theory_mean\n",
    "random_in_bin_next = np.random.randn(N_pts_in_bin)*sigma_corr + theory_mean\n",
    "\n",
    "\n",
    "    \n",
    "i = 0\n",
    "while (i < N_data_pts):\n",
    "    r_bin  = bin_width*(i+1)\n",
    "    theory_mean = gal_gal_cor_empricial_theory[(np.where(r == r_bin))[0] ]\n",
    "    ## inbclude correlations\n",
    "    random_new = np.random.randn(N_pts_in_bin)\n",
    "    \n",
    "    ## update the random arrays\n",
    "    random_in_bin_last = random_in_bin\n",
    "    random_in_bin = random_in_bin_next\n",
    "    random_in_bin_next = random_new\n",
    "      \n",
    "    ## compute the data point including correlation!!!!!!!! <<<<<<<<<<<<<<<<\n",
    "    data_in_bin = + correlatipon_fator*random_in_bin_next + random_in_bin + correlatipon_fator*random_in_bin_last\n",
    "    data_in_bin /= np.std(data_in_bin)  ## keep the standard devaition to be 1\n",
    "    data_in_bin = data_in_bin*sigma_corr + theory_mean\n",
    "        \n",
    "    mean_pt = np.mean(data_in_bin)\n",
    "    error_pt = np.std(data_in_bin) / np.sqrt(N_pts_in_bin - 1.) ## this is the error on the mean\n",
    "    plt.errorbar(r_bin,mean_pt,error_pt,marker=',',mec='red',mfc='red',c=\"red\")\n",
    "  \n",
    "    ## store the data for use below in a fit\n",
    "    Raw_data[i,:] = data_in_bin\n",
    "    Binned_r[i] = r_bin\n",
    "    Binned_mean[i] = mean_pt\n",
    "    Binned_error[i] = error_pt\n",
    "\n",
    "    ## iterate\n",
    "    i+=1\n",
    "\n",
    "plt.title(\"correlated\")\n",
    "plt.ylim(-.1,.2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250340cb",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "Compare the correlated vs no-correlated cases.  How do they look different?   How can you visually assess if signfiicnat correlations are present in a measurment?  Look at the first detection of BAO-- do you think there were signifcant correlations in this measurment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e93233",
   "metadata": {},
   "source": [
    " your discusison goes in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800ac54",
   "metadata": {},
   "source": [
    "## Fitting and Paramter estimation\n",
    "\n",
    "To fit we need a metrix of how well the data agree with the measurment.  Chi-squared provides just such a metric.   The definition of Chi-squared for uncorrelated data is $$ \\chi ^2 = \\Sigma_i \\frac{(y_i - f(x_i))^2}{\\sigma_i^2}.$$ Here $y_i$ are the data meaued as a function of some independant variable $x_i$ with variance $\\sigma_i$.  We can also rewrite this using matrix and vector notation as $$ \\chi ^2 = (\\vec y - \\vec f(x))^t N^{-2} ( \\vec y - \\vec f(x)).$$ In this equation $\\vec y$ is a vector containing each of the data points sampled at $\\vec x$, $\\vec f(x)$ is a model for this data vector, and $N^{-2}$ (in the case of uncorrelated data) is a diagonal matrix with elements $ N^{-2}_{ii} = \\frac 1 {\\sigma_i^2}$.  When we include correlations we will swich $ N^{-2}_{ii} = \\frac 1 {\\sigma_i\\sigma_j}$ to be the full covaraince matrix.\n",
    "\n",
    "We will use $\\chi^2$ in two ways.  First we can minimize it to find the bets fit.  In the examples below we will do a grid search-- there is an industry to minimization technques that are beyond the scope of this notebook.  Second, we can use $\\chi^2$ to place error elipses on our fit.  If we want the $n$-$\\sigma$ error eliplse, we just find where $\\Delta \\chi^2 = n^2$, where $\\Delta \\chi^2 = \\chi^2 - min(\\chi^2)$.  \n",
    "\n",
    "One way to understand the use of $\\Delta \\chi^2$ in setting paramter errors,  is that if we had a single data point then  $\\chi^2 = \\frac {(y - m)^2} {\\sigma^2}$.  In this case if the model $m$ is different from the data point $y$ by $n \\sigma$ then $\\chi^2$ would change by $n^2$ from its minima.   In the case of multiple data points this result can be generalized.  The can be easily checked by reruning a simulation of the measuremnt mutlipel times and looking at the variation in the simulated best fit parmaters.  These repeated simulations is the monty-carlo method.\n",
    "\n",
    "\n",
    "### fitting funcitons and fit by eye\n",
    "\n",
    "Here we define a fit funciton and attempt a fit by eye.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### define a fitting function. \n",
    "#followiong the disccion above we generate a fitting funciton which takes three varibales: \n",
    "    ## A the amplitude of the power law\n",
    "    ## B the ampltude of the gaussin feature represetning the BAO bump\n",
    "    ## r the central values of r at which the data are sampled\n",
    "def fit_funtion(A,B,r):\n",
    "    return( A*(1/10.)**(r/50.)+ B*np.exp(-0.5*((r-120)/10.)**2))  ## the funciton we used to \n",
    "\n",
    "\n",
    "##### define a chi_sqared funmciton\n",
    "## we will use the vector version and prepare for the case where correlations are present\n",
    "def chi_squared(binned_r,binned_mean, invCov,A,B):\n",
    "    model = fit_funtion(A,B,binned_r)\n",
    "    data_minus_model = binned_mean - model\n",
    "    chi_sq = np.dot( np.transpose(data_minus_model) , np.matmul(invCov, data_minus_model) )\n",
    "    return(chi_sq)\n",
    "    \n",
    "    \n",
    "\n",
    "plt.errorbar(Binned_r,Binned_mean,Binned_error,marker=',',mec='red',mfc='red',c=\"red\")\n",
    "plt.plot(Binned_r, fit_funtion(2,0.004,Binned_r))\n",
    "#plt.plot(Binned_r, fit_funtion(0.8,-0.01,Binned_r))\n",
    "#plt.plot(Binned_r, fit_funtion(1.3,0.04,Binned_r))\n",
    "plt.ylim(-.1,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f9346",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "adjust the paramters in the models aove to appriximate the 1 sigma errors for the model. Add a discsion to this cell.  These estimtes can be used for defining the search volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260b083",
   "metadata": {},
   "source": [
    "### carry out a fit ignoring correlations\n",
    "Since we stored the \"raw data\" that go into the above plot, we can work from that for computing the mean and the covarinace matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fit ignoring correlations\n",
    "\n",
    "### NOTE: the following quantities describing our simualted data were stored in the prvious cell:\n",
    "    #Raw_data -- the data whcih go into each bin on the plot, these include correlations\n",
    "    #Binned_r  -- the centra r of each bin\n",
    "    #Binned_mean  -- the mean in each bin\n",
    "    #Binned_error  -- the variance on the mean of each data point.\n",
    "\n",
    "## define a grid in A and B and compute a grid search.\n",
    "\n",
    "A_points = np.arange(0.97,1.05,.001)\n",
    "B_points = np.arange(0.0,.04,.0005)\n",
    "N_pts_A = np.size(A_points)\n",
    "N_pts_B = np.size(B_points)\n",
    "chi_sq_map = np.zeros([N_pts_A,N_pts_B])\n",
    "\n",
    "## define a covariance mattix \n",
    "invCov_nocor = np.diag(Binned_error**(-2))\n",
    "\n",
    "\n",
    "## compute chi_squared for each point\n",
    "i = 0\n",
    "while (i < N_pts_A):\n",
    "    j = 0\n",
    "    while (j < N_pts_B):\n",
    "        chi_sq_map[i,j] = chi_squared(Binned_r,Binned_mean, invCov_nocor,A_points[i],B_points[j])\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "\n",
    "    \n",
    "######\n",
    "### make a plot of the 1,2,..5 sigma error contours on the fit, \n",
    "### show the \"truth\" from the simulations, and \n",
    "### show the best fit.\n",
    "\n",
    "## make a plot of chi_sq\n",
    "delta_chi_sq_map = chi_sq_map - np.min(chi_sq_map)\n",
    "plt.contour(B_points,A_points,delta_chi_sq_map,levels=np.array([1,2**2,3**2,4**2,5**2]))\n",
    "\n",
    "\n",
    "## plot the best fit\n",
    "pos_min = np.where(chi_sq_map == np.min(chi_sq_map))\n",
    "plt.plot(B_points[pos_min[1]],A_points[pos_min[0]],'gd')\n",
    "print(A_points[pos_min[0]],B_points[pos_min[1]],np.min(chi_sq_map),np.size(Binned_r))\n",
    "\n",
    "\n",
    "## plot the \"true\" parmaters (eg what we used in the simulation)\n",
    "plt.plot(np.array(0.02),np.array(1),'ro')\n",
    "plt.xlabel(\"B-- the amplitude of (air quotes) BAO feature\")\n",
    "plt.ylabel(\"A-- the amplitude power law part of our correlation function \")\n",
    "plt.title(\"red dot = simuled value, green diamond = best fit, 1-5 sigma error eplipses shown\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae71dd",
   "metadata": {},
   "source": [
    "### covarince matrices\n",
    "\n",
    "In the above fit we used a diagnoal covarince matrix which ignores correlations.  Here we will visualize this covariance matrix before calculating the full covarince and repeating the fit using the full covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5de050",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.where(invCov_nocor == 0)\n",
    "cov_matrix = 1./invCov_nocor\n",
    "cov_matrix[mask] = 0.\n",
    "\n",
    "plt.imshow(cov_matrix,vmin = 0, vmax = np.max(cov_matrix))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621f1d5",
   "metadata": {},
   "source": [
    "Note how only the diagonal values are non-zero.\n",
    "\n",
    "\n",
    "### covaraince matrix\n",
    "\n",
    "We will now compute the covarince matrix accounting for correlations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the correlation matrix \n",
    "\n",
    "### NOTE: the following quantities describing our simualted data were stored in the prvious cell:\n",
    "    #Raw_data -- the data whcih go into each bin on the plot, these include correlations\n",
    "    #Binned_r  -- the centra r of each bin\n",
    "    #Binned_mean  -- the mean in each bin\n",
    "    #Binned_error  -- the variance on the mean of each data point.\n",
    "    \n",
    "    \n",
    "cov = np.zeros([np.size(Binned_r),np.size(Binned_r)])  ## make a matrix to hold the covariance\n",
    "\n",
    "\n",
    "## compute the covarinace from the Raw data in each bin\n",
    "i = 0\n",
    "while (i < np.size(Binned_r)):\n",
    "    j = 0\n",
    "    while (j < np.size(Binned_r)):\n",
    "        di = Raw_data[i,:]-np.mean(Raw_data[i,:])\n",
    "        dj = Raw_data[j,:]-np.mean(Raw_data[j,:])\n",
    "        cov[i,j] = ( np.sum(di*dj) / (N_pts_in_bin**2))   ## note how easy this is.  The N_pts_in_bin^2 factor is so we get the covariance on the mean\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "\n",
    "## plot the covaraince matrix.\n",
    "plt.imshow(cov)\n",
    "plt.title(\"covaraince matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac970a",
   "metadata": {},
   "source": [
    "### Excercise:\n",
    "Compute the correlation matrix for these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ce1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43425f4f",
   "metadata": {},
   "source": [
    "Computing  $\\chi^2$ requires the inverse of the covaraince matrix.  Given thave covariance matrices often have singular (zero) eigenvalues we will use the singular value decompositin to carry out the inversion.  If any of the eigenvalues are found to be zero these could be regulerized.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the determinant of the covariance matrix-- if this is small it may be a sign of a signular eigenvalue\n",
    "print(\"det(cov)\",np.linalg.det(cov))\n",
    "\n",
    "print(\"the small value looks scarry-- lets see if this is dominated by one eigenvalue by printing these out\")\n",
    "u,s,v=np.linalg.svd(cov)\n",
    "print(s)\n",
    "print(\"since all eigenvalues are approximatly the same size we can safely invert this matrix\")\n",
    "inv_cov=np.dot(v.transpose(),np.dot(np.diag(s**-1),u.transpose()))\n",
    "\n",
    "## plot the inverse covaraince matrix\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "plt.imshow(inv_cov)\n",
    "plt.title(\"inverse covaraince matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## optional check to verify the inversion worked\n",
    "#test = np.matmul(inv_cov,cov)\n",
    "#plt.imshow(test)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a853ba",
   "metadata": {},
   "source": [
    "## now repeat the fit using this full inverse covaraince to compute Chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcebc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## compute chi_squared for each point\n",
    "i = 0\n",
    "while (i < N_pts_A):\n",
    "    j = 0\n",
    "    while (j < N_pts_B):\n",
    "        chi_sq_map[i,j] = chi_squared(Binned_r,Binned_mean, inv_cov,A_points[i],B_points[j])\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "delta_chi_sq_map = chi_sq_map - np.min(chi_sq_map)\n",
    "\n",
    "   \n",
    "######\n",
    "### make a plot of the 1,2,..5 sigma error contours on the fit, \n",
    "### show the \"truth\" from the simulations, and \n",
    "### show the best fit.\n",
    "\n",
    "## make a plot of chi_sq\n",
    "delta_chi_sq_map = chi_sq_map - np.min(chi_sq_map)\n",
    "plt.contour(B_points,A_points,delta_chi_sq_map,levels=np.array([1,2**2,3**2,4**2,5**2]))\n",
    "\n",
    "\n",
    "## plot the best fit\n",
    "pos_min = np.where(chi_sq_map == np.min(chi_sq_map))\n",
    "plt.plot(B_points[pos_min[1]],A_points[pos_min[0]],'gd')\n",
    "print(A_points[pos_min[0]],B_points[pos_min[1]],np.min(chi_sq_map),np.size(Binned_r))\n",
    "\n",
    "\n",
    "## plot the \"true\" parmaters (eg what we used in the simulation)\n",
    "plt.plot(np.array(0.02),np.array(1),'ro')\n",
    "plt.xlabel(\"B-- the amplitude of (air quotes) BAO feature\")\n",
    "plt.ylabel(\"A-- the amplitude power law part of our correlation function \")\n",
    "plt.title(\"red dot = simuled value, green diamond = best fit, 1-5 sigma error eplipses shown\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a628542",
   "metadata": {},
   "source": [
    "## Notes: \n",
    "\n",
    "1. Including the correlations through the covariance changed the bet fit and the erorr bars.  Ignoring correlations in fitting data can lead to incorrect results.\n",
    "\n",
    "1. The significnace of the detector of the \"B\" paramter (e.g., the \"BAO\" bump amplitude) changed when correlations were includede--  in the case stored in this notebook the detetion signficance decreased to below 4 sigma compared to > 5 when corelations were ignored.   Correlations are critical for drawing conclusions from data.\n",
    "\n",
    "1. The error elipises computing with correlations have a tilt indicating that the errors on the $A$ and $B$ paramters are correlated.\n",
    "\n",
    "1. You can rerun this notebook multiple times to see how different random variations in the measuremnt with the same statistical weight can cahnge the results.  Note that one realization my be 5 sigma, but others could be higher or lower.   This variation in the detection signficance is one of the reasons 5 sigma is addopted as a critical threshold for claiming a detection.\n",
    "\n",
    "1. The method we used here is direct and theirfore straightforward to understand.  A huge body of methods exist to speed up fits and estimate of errors on fit paramters.  These include monty-carlo methods, Fisher information mateix (second derivitve of Chi-sq), and others.  This notebook shold give you a framework for thinking about what is going on in these methods. \n",
    "\n",
    "1. many other types of noise correaltions exist.  Below we will play with a different example as an excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = np.arange(6000)\n",
    "\n",
    "def toy_CMB_TT(ell,ns,r):\n",
    "    Dl_toy= 20 + 5* np.cos(2*np.pi / 400*ell)/2.\n",
    "    Dl_toy*= ell*(ell+1)  /10000.\n",
    "    Dl_toy+= 10000* np.exp(-.5*((ell+300)/100)**2 )\n",
    "    Dl_toy+= 5000* np.exp(-.5*((ell-200)/75)**2 )\n",
    "    Dl_toy+= 1000* np.exp(-.5*((ell-500)/50)**2 )\n",
    "    Dl_toy+= 400* np.exp(-.5*((ell-800)/50)**2 )\n",
    "    Dl_toy*= (1/10)**((ell)/1000)\n",
    "    Dl_toy*= (ell / 500.)**(-(ns-1.))\n",
    "    Dl_toy+= 10000* np.exp(-.5*((ell+300)/100)**2 )*r\n",
    "    return(Dl_toy)\n",
    "\n",
    "\n",
    "plt.loglog(ell,toy_CMB_TT(ell,1,0),\"r\")\n",
    "plt.loglog(ell,toy_CMB_TT(ell,0.9,0),\"b\")\n",
    "plt.loglog(ell,toy_CMB_TT(ell,1,1),\"g\")\n",
    "plt.title(\"completey made up CMB power spectrum\")\n",
    "plt.xlabel(\"multiplole moment $(\\ell)$\")\n",
    "plt.ylabel(\"$D_\\ell^{TT}$ [$\\mu$ K$^2$]\")\n",
    "plt.xlim(10,5000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f287ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make fake data\n",
    "\n",
    "ell_measure = np.arange(10,3000,10)\n",
    "DL_sim = toy_CMB_TT(ell_measure,0.96,0)\n",
    "N_bins = np.size(ell_measure)\n",
    "N_pts_in_bin = 1000\n",
    "## arrays to store the results for future use\n",
    "Raw_data = np.zeros([N_bins,N_pts_in_bin])\n",
    "Binned_mean = np.zeros(N_bins)\n",
    "Binned_error = np.zeros(N_bins)\n",
    "\n",
    "plt.loglog(ell_measure,DL_sim)\n",
    "\n",
    "\n",
    "## make a model of the beam calibraiton error-- this will lead to the correlated error\n",
    "beam_sigma = 2./60. *np.pi/180. /np.sqrt(8*np.log(2))\n",
    "beam= np.exp(-ell_measure * (ell_measure+1)*beam_sigma**2)\n",
    "beam_corr= np.exp(-ell_measure * (ell_measure+1)*(beam_sigma*0.99)**2)\n",
    "  #plt.plot(ell_measure,beam-beam_corr)\n",
    "beam_error = (beam-beam_corr)\n",
    "\n",
    "## make an evelovpe of the error vs ell from sample variance of the sky\n",
    "N_SV = 1. / (ell_measure*(ell_measure+1)) * DL_sim*100\n",
    "## make an evelovpe of the error vs ell from sample variance of the sky\n",
    "N_beam = 1. / beam\n",
    "\n",
    "#error uncorrelated envelope\n",
    "Noise_uncorrelated = N_SV + N_beam\n",
    "\n",
    "noise_amplitude = 50.\n",
    "\n",
    "\n",
    "beam_random = np.random.randn(N_pts_in_bin)\n",
    "\n",
    "i = 0\n",
    "while (i < N_bins):\n",
    "    data_in_bin = np.random.randn(N_pts_in_bin) * Noise_uncorrelated[i]* noise_amplitude\n",
    "    data_in_bin += beam_random * beam_error[i]*DL_sim[i]*100\n",
    "    data_in_bin += DL_sim[i]\n",
    "    \n",
    "        \n",
    "    mean_pt = np.mean(data_in_bin)\n",
    "    error_pt = np.std(data_in_bin) / np.sqrt(N_pts_in_bin - 1.) ## this is the error on the mean\n",
    "    plt.errorbar(ell_measure[i],mean_pt,error_pt,marker=',',mec='red',mfc='red',c=\"red\")\n",
    "  \n",
    "    ## store the data for use below in a fit\n",
    "    Raw_data[i,:] = data_in_bin\n",
    "    Binned_mean[i] = mean_pt\n",
    "    Binned_error[i] = error_pt\n",
    "    ## note ell_measure is also needed\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "plt.loglog(ell,toy_CMB_TT(ell,0.9,0),\"r\")\n",
    "plt.loglog(ell,toy_CMB_TT(ell,1,1),\"y\")\n",
    "plt.xlim(10,3000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd021c9",
   "metadata": {},
   "source": [
    "## Excercise \n",
    "\n",
    "Use the tecniques above to fit ns and r from thsi toy model of the temperture spectrum toy model presented here.  \n",
    "\n",
    "NOTE: to do thsi correctly you could generate a complete noise model, use CAMB to generate the theory (rahter than our toy model), and then sample the paratmer space using a montycarlo method.  This toy model illustrates different types of correlations from what was presented earlier, and give you a chance to practice the technqiues of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d53ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
